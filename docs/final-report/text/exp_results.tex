Since the quality of the output of our solution is defined
in highly subjective terms, we couldn't use any quantitative metric
directly to evaluate our results. Hence we did user evaluation by reading
the papers and rating the presentations generated for those papers.
We selected a set of 10 papers randomly, and gave scores (out of 10) on following 
parameters:
\begin{itemize}
	\item Q1: Information coverage by presentation
	\item Q2: Coherence between slides
	\item Q3: Closeness to the final presentation
	\item Q4: Overall satisfaction
\end{itemize}
The average response is summarized in Table~\ref{tbl1:users}.

\begin{table}
\begin{center}
\caption{User Satisfaction Survey}
\begin{tabular}{| l | l | l | l | l |}
\hline
User & Q1 & Q2 & Q3 & Q4 \\ \hline
User 1 & 9 & 9 & 8 & 9 \\ 
User 2 & 9 & 7 & 7 & 8 \\ 
User 3 & 10 & 8 & 7 & 9 \\ \hline
Average & 9.33 & 8 & 7.33 & 8.33 \\ \hline
\end{tabular}
\label{tbl1:users}
\end{center}
\end{table}	


\subsection{Inter-Judge Similarity}
We use the Fleiss' $\kappa$ measure \cite{kappa} to estimate the inter judge similarity
(since the common Cohen's $\kappa$ measure only works for binary categorization
of input, whereas we want to give scores out of 10).
Each cell of the table gives the number of users who gave score $i$ on 
parameter $j$. Let $N = 4$ be the total number of papers considered
and $k = 10$ be the number of ratings for each parameters, number of raters $n = 3$, 
and let $n_{ij}$
represents number of raters who gave score $j$ to paper $i$.
For each row, $P_j$, proportion of papers getting $j^{th}$ score, is defined as:
\[
	P_j = \frac{1}{Nn} \sum_{i=1}^{N} n_{ij}
\]
For each column, $P_i$ gives the extent to which the raters agree for $i^{th}$ subject
\[
	P_i = \frac{1}{n(n-1)} \left[ \left( \sum_{j=1}^{k} n_{ij}^{2} \right) - \left( n \right) \right]
\]
The above values are summarized in Table~\ref{tbl1:kappa}.
Now, to calculate $\kappa$, we need,
\[
	\bar{P} = \frac{1}{N} \sum_{i=1}^{N} P_i = \frac{1}{4} \left( 0.33 + 0 + 1 + 0.33 \right) = 0.415
\]
and
\[
	\bar{P_e} = \sum_{1}^{k} P_j^2 = 0 + 0.1089 + 0.1681 + 0.00689 = 0.2838
\]
Hence, 
\[
	\kappa = \frac {\bar{P} - \bar{P_e}} {1 - \bar{P_e}} = \frac{0.415 - 0.283}{1 - 0.283} = 0.1831
\]
This value of $\kappa$ shows there was slight agreement between the judges as per \cite{kappa}.

\begin{table}
\begin{center}
\caption{Inter-Judge Agreement}
\begin{tabular}{| l || l | l | l | l || l | l |}
\hline
Score & P1 & P2 & P3 & P4 & Total & $P_j$\\ \hline
1 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
2 & 0 & 0 & 0 & 0 & 0 & 0 \\
3 & 0 & 0 & 0 & 0 & 0 & 0 \\
4 & 0 & 0 & 0 & 0 & 0 & 0 \\
5 & 0 & 0 & 0 & 0 & 0 & 0 \\
6 & 0 & 0 & 0 & 0 & 0 & 0 \\
7 & 1 & 1 & 0 & 2 & 4 & 0.33 \\
8 & 2 & 0 & 3 & 0 & 5 & 0.41 \\
9 & 0 & 1 & 0 & 1 & 2 & 0.16 \\
10 & 0 & 1 & 0 & 0 & 1 & 0.083 \\ \hline
$P_i$ & 0.33 & 0 & 1 & 0.33 &  &  \\ \hline
\end{tabular}
\label{tbl1:kappa}
\end{center}
\end{table}	
